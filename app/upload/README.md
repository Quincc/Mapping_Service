# UploadÂ module (app/upload)

## ðŸ“ŒÂ Purpose

Handles **ingress of raw data files** into the mapping-service:

* Accepts multiâ€‘part uploads from the browser / cURL
* Can pull a file from an S3â€‘compatible bucket
* Saves the payload into `<UPLOAD_DIR>/<project_id>/` under a collisionâ€‘free UUID filename
* Emits structured events (`file_received`, `file_processed`, â€¦)
* Launches the full ETL pipeline in a FastAPI backgroundâ€‘task

---

## ðŸŒÂ Public API

| Method | Route           | Body / Params                | Auth                        | Description                                             |
| ------ | --------------- | ---------------------------- | --------------------------- | ------------------------------------------------------- |
| `POST` | `/upload/local` | `multipart/formâ€‘data`Â `file` | `X-PROJECT-ID`, `X-API-KEY` | Saves an uploaded file, returns `201` + path, fires ETL |

> **Auth** comes from the common dependency `get_current_project`.

Response example

```jsonc
{
  "detail": "file_saved",
  "path": "/abs/path/to/uploads/<project_id>/<uuid>.csv"
}
```

---

## ðŸ“‚Â Directory layout

```
app/upload/
â”œâ”€â”€ router.py     # FastAPI endpoints
â”œâ”€â”€ service.py    # I/O: local FS + S3
â”œâ”€â”€ tasks.py      # background wrapper around run_full_pipeline()
â”œâ”€â”€ utils.py      # publish_event()
â””â”€â”€ schemas.py    # Pydantic DTOs
```

---

## ðŸ› Â Key components

### service.py

* **store\_local\_file** â€“ streams the `UploadFile` to disk in 1Â MiB chunks; validates extension (see `ALLOWED_EXT`).
* **store\_s3\_object** â€“ downloads an object via `boto3` and stores it in the same project directory.
* Both functions publish `file_received` immediately after the file hits the disk.

### tasks.py

```text
launch_pipeline â†’ run_full_pipeline â†’ publish file_processed/file_failed
```

Keeps HTTP handler lean; ideal place for timers, Prometheus metrics or retryâ€‘logic if needed later.

### utils.py

Thin wrapper around `structlog` so that changing the transport (Kafka, NATS, â€¦) wonâ€™t touch business code.

---

## âš™ï¸Â Configuration

| Variable               | Purpose                                                        | Example                              | Default            |
| ---------------------- | -------------------------------------------------------------- | ------------------------------------ | ------------------ |
| `UPLOAD_DIR`           | Root directory for stored files                                | `./uploads`                          | `./uploads`        |
| `ALLOWED_EXT`          | Commaâ€‘separated whitelist of extensions (lowerâ€‘case, with dot) | `.csv,.xlsx,.json`                   | `.csv,.xlsx,.json` |
| `S3_ENDPOINT`          | URL of the S3â€‘compatible service                               | `https://s3.usâ€‘eastâ€‘1.amazonaws.com` | â€”                  |
| `S3_REGION`            | Region string                                                  | `usâ€‘eastâ€‘1`                          | â€”                  |
| `S3_KEY` / `S3_SECRET` | Access credentials                                             | â€”                                    | â€”                  |

All variables are read via **core/settings.py** (Pydantic settings).

---

## ðŸš¦Â Event flow

```mermaid
sequenceDiagram
    Browser â†’> API: POST /upload/local (file)
    API â†’> service.store_local_file: stream file
    service -->> utils.publish_event: file_received
    API -->> Browser: 201 Created
    API âž¤âž¤ bg task: launch_pipeline()
    launch_pipeline â†’> ETL: run_full_pipeline
    run_full_pipeline -->> publish_event: file_processed / file_failed
```

Events are logged as structured JSON; can be piped to Loki/OpenSearch or any log shipper.

---

## ðŸ§©Â Extending

* **Chunked uploads** â€“ add a new route `/upload/multipart/init|part|complete` and store parts to temp dir.
* **Virus scan** â€“ call a ClamAV microâ€‘service right after `store_local_file`.
* **Cloud storage only** â€“ swap out `store_local_file` for an S3 `put_object` and store just the URI.

---

## ðŸ§ªÂ Testing hints

* Use **pytest + respx** to stub S3.
* For filesystem tests rely on `tmp_path` fixture: pass it via `monkeypatch.setattr(settings, "UPLOAD_DIR", tmp_path)`.
* Assert that `publish_event` is called (patch with `unittest.mock.AsyncMock`).

---

Â©Â Mappingâ€‘Service team, 2025
